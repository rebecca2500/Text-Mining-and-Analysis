{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'read',\n",
       " 'a',\n",
       " 'book',\n",
       " 'i',\n",
       " 'read',\n",
       " 'a',\n",
       " 'different',\n",
       " 'book',\n",
       " 'he',\n",
       " 'read',\n",
       " 'a',\n",
       " 'book',\n",
       " 'my',\n",
       " 'mulan']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import pad_sequence\n",
    "\n",
    "\n",
    "text=open('Text Corpus.txt').read()\n",
    "\n",
    "#tokenize the text\n",
    "tokens = nltk.tokenize.word_tokenize(text.lower())\n",
    "filtered=[]\n",
    "padding=['/s','<','>','s']\n",
    "\n",
    "#removing <s> and </s> from the original text\n",
    "for w in tokens:\n",
    "    if w not in padding:\n",
    "        filtered.append(w)\n",
    "\n",
    "\n",
    "#add padding to the text\n",
    "filteredTokens= list(pad_sequence(filtered, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=1))\n",
    "\n",
    "# n = 1 because unigram\n",
    "n = 1 \n",
    "train_data, padded_sents = padded_everygram_pipeline(n, filteredTokens)\n",
    "train_data2, padded_sents2 = padded_everygram_pipeline(n, filteredTokens)\n",
    "\n",
    "#train unsmoothed model with MLE\n",
    "unsmoothed = MLE(n) \n",
    "unsmoothed.fit(train_data, padded_sents)\n",
    "\n",
    "#train smoothed model with Laplace\n",
    "smoothed = Laplace(n)\n",
    "smoothed.fit(train_data2, padded_sents2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsmoothed Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13333333333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('he') # P('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('read') # P('read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('a') # P('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('book') # P('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('i') # P('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('different') # P('different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('my')  # P('my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('mulan')# P('mulan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplance Smoothing Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score('he') # P('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score('read') # P('read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score('a') # P('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score('book') # P('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score('i')  # P('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score('different')  # P('different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score('my') # P('my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score('mulan') # P('mulan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "\n",
    "# Import the file\n",
    "text = open ('Text Corpus.txt')\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = nltk.tokenize.word_tokenize((text.read()).lower())\n",
    "\n",
    "# Do padding to remove <s> and <s/s> from original text\n",
    "padding = ['s','<','>','/s']\n",
    "filtered = []\n",
    "\n",
    "for w in tokenized_text:\n",
    "    if w not in padding:\n",
    "        filtered.append (w)\n",
    "filteredtokens = [list(filtered)]\n",
    "\n",
    "# n=2 for Bigram\n",
    "n = 2\n",
    "train_data, padded_sents = padded_everygram_pipeline (n, filteredtokens)\n",
    "train_data2, padded_sents2 = padded_everygram_pipeline (n, filteredtokens)\n",
    "\n",
    "# Training Unsmoothed Bigram Probability\n",
    "model = MLE(n)\n",
    "model.fit (train_data, padded_sents)\n",
    "\n",
    "# Training Smoothed Bigram Probability\n",
    "model2 = Laplace(n)\n",
    "model2.fit (train_data2, padded_sents2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsmoothed Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score('read', 'he'.split())  # P('read'|'he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score('a', 'read'.split())  # P('a'|'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score('book', 'a'.split())  # P('book'|'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score('read', 'i'.split())  # P('read'|'I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score('different','a'.split()) # P('different'|'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score('book','different'.split()) # P('book'|'different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score('my', 'book'.split())  # P('my'|'book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score('mulan', 'my'.split())  # P('mulan'|'my')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace Smoothed Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('read', 'he'.split())  # P('read'|'he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('he', '<s>'.split())  # P('he'|'<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('a', 'read'.split())  # P('a'|'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('book', 'a'.split())  # P('book'|'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('read', 'i'.split())  # P('read'|'I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('</s>', 'book'.split())  # P('</s>'|'book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('i', '<s>'.split())  # P('i'|'<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('different','a'.split()) # P('different'|'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('book','different'.split()) # P('book'|'different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('my', 'book'.split())  # P('my'|'book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('mulan', 'my'.split())  # P('mulan'|'my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score('</s>', 'mulan'.split())  # P('read'|'he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Sentence Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005787037037037037"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P (W= He read a book)\n",
    "model2.score('he')*model2.score('read')*model2.score('a')*model2.score('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.215020576131687e-05"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P (W= I read a different book)\n",
    "model2.score('i')*model2.score('read')*model2.score('a')*model2.score('different')*model2.score('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0187757201646085e-06"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P (W= He read a book my mulan)\n",
    "model2.score('he')*model2.score('read')*model2.score('a')*model2.score('book')*model2.score('my')*model2.score('mulan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Sentence Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'He read a book'\n",
    "model2.score('he', '<s>'.split()) * model2.score('read', 'he'.split()) * model2.score('a', 'read'.split()) * model2.score('book', 'a'.split()) * model2.score('</s>', 'book'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'I read a different book'\n",
    "model2.score('i', '<s>'.split()) * model2.score('read', 'i'.split()) * model2.score('a', 'read'.split()) * model2.score('different', 'a'.split()) * model2.score('book', 'different'.split()) * model2.score('</s>', 'book'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'He read a book my mulan'\n",
    "model2.score('he', '<s>'.split()) * model2.score('read', 'he'.split()) * model2.score('a', 'read'.split()) * model2.score('book', 'a'.split()) * model2.score('my', 'book'.split()) * model2.score('mulan', 'my'.split()) * model2.score('</s>', 'mulan'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "#to read the csv using pandas\n",
    "file = pd.read_csv('Musical_Instruments_Reviews.csv')\n",
    "\n",
    "#function to calculate the polarity\n",
    "def polarity(sentence):\n",
    "    polarity_score = TextBlob(sentence).polarity;\n",
    "    return polarity_score\n",
    "# function to categorize sentiment based on polarity value\n",
    "def sentiment(polarity):\n",
    "    if (polarity==0):\n",
    "        return \"neutral\"\n",
    "    elif (polarity<0):\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "#add polarity column to the csv file\n",
    "file['polarity']=file['Reviews'].apply(polarity)\n",
    "#add sentiment column to the csv file\n",
    "file['sentiment']=file['polarity'].apply(sentiment)\n",
    "#count the number of positive, negative and neutral sentiment\n",
    "file['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.to_csv('./MusicalSentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv ('MusicalSentiment.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize word\n",
    "textfile=[(word_tokenize(column[\"Reviews\"]),column[\"sentiment\"])for index,column in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique set from the sentiment\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in unique_list:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "unique_set = set ();\n",
    "\n",
    "for (review,sentiment) in textfile:\n",
    "    for word in review:\n",
    "        unique_set.add (word.lower())\n",
    "\n",
    "unique_list = list (unique_set);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply feature\n",
    "from nltk.classify.util import apply_features\n",
    "features = apply_features(find_features,textfile,labeled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test data with 80:20 ratio\n",
    "\n",
    "train_test_split = round(len(features)*0.8)\n",
    "train_set = features[:train_test_split]\n",
    "test_set = features [train_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classification to count performance measure\n",
    "\n",
    "from nltk import NaiveBayesClassifier\n",
    "NaiveBayes = nltk.NaiveBayesClassifier.train(train_set)\n",
    "import collections\n",
    "reference = collections.defaultdict(set)\n",
    "test = collections.defaultdict(set)\n",
    "\n",
    "for N, (review,sentiment) in enumerate (test_set):\n",
    "    reference [sentiment].add (N)\n",
    "    observation = NaiveBayes.classify(review)\n",
    "    test[observation].add (N)\n",
    "    \n",
    "print (\"Accuracy is :\" , nltk.classify.accuracy(NaiveBayes,test_set)*100)\n",
    "print (\"Precision is :\" , nltk.scores.precision(reference[\"Positive\"],test[\"Positive\"]))\n",
    "print (\"Recall is :\" , nltk.scores.recall(reference[\"Positive\"],test[\"Positive\"]))\n",
    "print (\"F1 scores is :\" , nltk.scores.f_measure(reference[\"Positive\"],test[\"Positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> DT AA\n",
    "VP -> V  NP\n",
    "AA -> Adj AA|Adj NN\n",
    "\n",
    "NN -> 'bear' | 'squirrel'\n",
    "Adj -> 'buster'| 'angry'| 'frightened'| 'little'\n",
    "V -> 'chased' \n",
    "DT -> 'the'\n",
    "\n",
    "\"\"\")\n",
    "file = 'the buster angry bear chased the frightened little squirrel'\n",
    "\n",
    "wordtokens = nltk.tokenize.word_tokenize(file)\n",
    "\n",
    "parser = nltk.ChartParser(text)\n",
    "for tree in parser.parse(wordtokens):\n",
    "    tree.draw()\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
