{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  8 sentences in this text\n",
      "\n",
      "1 . You probably worked out that a backslash means that the following character is deprived of its special powers and must literally match a specific character in the word. \n",
      "\n",
      "2 . Thus, while . \n",
      "\n",
      "3 . is special, \\. \n",
      "\n",
      "4 . only matches a period. \n",
      "\n",
      "5 . The braced expressions, like {3,5}, specify the number of repeats of the previous item. \n",
      "\n",
      "6 . The pipe character indicates a choice between the material on its left or its right. \n",
      "\n",
      "7 . Parentheses indicate the scope of an operator: they can be used together with the pipe (or disjunction) symbol like this: «w(i|e|ai|oo)t», matching wit, wet, wait, and woot. \n",
      "\n",
      "8 . It is instructive to see what happens when you omit the parentheses from the last expression above. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "file = open(\"Data_1.txt\").read() #read txt file\n",
    "Sentences = sent_tokenize(file) #sentence tokenization\n",
    "\n",
    "\n",
    "print(\"There are \", len(Sentences), \"sentences in this text\\n\")\n",
    "# print the sentence that has been tokenize\n",
    "counter = 0\n",
    "for sent in Sentences:\n",
    "    counter+=1\n",
    "    print(counter,\".\",sent,\"\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisation using split function:\n",
      " \n",
      "There are  111 tokens in this text\n",
      "\n",
      "['You', 'probably', 'worked', 'out', 'that', 'a', 'backslash', 'means', 'that', 'the', 'following', 'character', 'is', 'deprived', 'of', 'its', 'special', 'powers', 'and', 'must', 'literally', 'match', 'a', 'specific', 'character', 'in', 'the', 'word.', 'Thus,', 'while', '.', 'is', 'special,', '\\\\.', 'only', 'matches', 'a', 'period.', 'The', 'braced', 'expressions,', 'like', '{3,5},', 'specify', 'the', 'number', 'of', 'repeats', 'of', 'the', 'previous', 'item.', 'The', 'pipe', 'character', 'indicates', 'a', 'choice', 'between', 'the', 'material', 'on', 'its', 'left', 'or', 'its', 'right.', 'Parentheses', 'indicate', 'the', 'scope', 'of', 'an', 'operator:', 'they', 'can', 'be', 'used', 'together', 'with', 'the', 'pipe', '(or', 'disjunction)', 'symbol', 'like', 'this:', '«w(i|e|ai|oo)t»,', 'matching', 'wit,', 'wet,', 'wait,', 'and', 'woot.', 'It', 'is', 'instructive', 'to', 'see', 'what', 'happens', 'when', 'you', 'omit', 'the', 'parentheses', 'from', 'the', 'last', 'expression', 'above.']\n",
      "\n",
      "Tokenisation using nltk word tokenise function :\n",
      " \n",
      "There are  138 tokens in this text\n",
      "\n",
      "['You', 'probably', 'worked', 'out', 'that', 'a', 'backslash', 'means', 'that', 'the', 'following', 'character', 'is', 'deprived', 'of', 'its', 'special', 'powers', 'and', 'must', 'literally', 'match', 'a', 'specific', 'character', 'in', 'the', 'word', '.', 'Thus', ',', 'while', '.', 'is', 'special', ',', '\\\\', '.', 'only', 'matches', 'a', 'period', '.', 'The', 'braced', 'expressions', ',', 'like', '{', '3,5', '}', ',', 'specify', 'the', 'number', 'of', 'repeats', 'of', 'the', 'previous', 'item', '.', 'The', 'pipe', 'character', 'indicates', 'a', 'choice', 'between', 'the', 'material', 'on', 'its', 'left', 'or', 'its', 'right', '.', 'Parentheses', 'indicate', 'the', 'scope', 'of', 'an', 'operator', ':', 'they', 'can', 'be', 'used', 'together', 'with', 'the', 'pipe', '(', 'or', 'disjunction', ')', 'symbol', 'like', 'this', ':', '«', 'w', '(', 'i|e|ai|oo', ')', 't', '»', ',', 'matching', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'It', 'is', 'instructive', 'to', 'see', 'what', 'happens', 'when', 'you', 'omit', 'the', 'parentheses', 'from', 'the', 'last', 'expression', 'above', '.']\n",
      "\n",
      "Tokenisation using RE word tokenise function :\n",
      " \n",
      "There are  115 tokens in this text\n",
      "\n",
      "['You', 'probably', 'worked', 'out', 'that', 'a', 'backslash', 'means', 'that', 'the', 'following', 'character', 'is', 'deprived', 'of', 'its', 'special', 'powers', 'and', 'must', 'literally', 'match', 'a', 'specific', 'character', 'in', 'the', 'word', 'Thus', 'while', 'is', 'special', 'only', 'matches', 'a', 'period', 'The', 'braced', 'expressions', 'like', '3', '5', 'specify', 'the', 'number', 'of', 'repeats', 'of', 'the', 'previous', 'item', 'The', 'pipe', 'character', 'indicates', 'a', 'choice', 'between', 'the', 'material', 'on', 'its', 'left', 'or', 'its', 'right', 'Parentheses', 'indicate', 'the', 'scope', 'of', 'an', 'operator', 'they', 'can', 'be', 'used', 'together', 'with', 'the', 'pipe', 'or', 'disjunction', 'symbol', 'like', 'this', 'w', 'i', 'e', 'ai', 'oo', 't', 'matching', 'wit', 'wet', 'wait', 'and', 'woot', 'It', 'is', 'instructive', 'to', 'see', 'what', 'happens', 'when', 'you', 'omit', 'the', 'parentheses', 'from', 'the', 'last', 'expression', 'above']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# read txt file\n",
    "file = open(\"Data_1.txt\").read()\n",
    "#word tokenization using split\n",
    "wordtokens = file.split(' ')\n",
    "print(\"Tokenisation using split function:\\n \")\n",
    "print(\"There are \", len(wordtokens), \"tokens in this text\\n\")\n",
    "print(wordtokens)\n",
    "#word tokenization using nltk\n",
    "print(\"\\nTokenisation using nltk word tokenise function :\\n \")\n",
    "print(\"There are \", len(nltk.tokenize.word_tokenize(file)), \"tokens in this text\\n\")\n",
    "print(nltk.tokenize.word_tokenize(file))\n",
    "#word tokenization using regular expression \n",
    "print(\"\\nTokenisation using RE word tokenise function :\\n \")\n",
    "re_words = re.findall(r\"[\\'\\w\\-]+\",file)\n",
    "print(\"There are \", len(re_words), \"tokens in this text\\n\")\n",
    "print(re_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word stemming using regular expression :\n",
      "\n",
      "['You', 'probab', 'work', 'out', 'that', 'a', 'backslash', 'mean', 'that', 'the', 'follow', 'character', 'i', 'depriv', 'of', 'it', 'special', 'power', 'and', 'must', 'literal', 'match', 'a', 'specific', 'character', 'in', 'the', 'word', '.', 'Thu', ',', 'while', '.', 'i', 'special', ',', '\\\\', '.', 'on', 'match', 'a', 'period', '.', 'The', 'brac', 'expression', ',', 'like', '{', '3,5', '}', ',', 'specify', 'the', 'number', 'of', 'repeat', 'of', 'the', 'prev', 'item', '.', 'The', 'pipe', 'character', 'indicat', 'a', 'choice', 'between', 'the', 'material', 'on', 'it', 'left', 'or', 'it', 'right', '.', 'Parenthes', 'indicate', 'the', 'scope', 'of', 'an', 'operator', ':', 'they', 'can', 'be', 'us', 'together', 'with', 'the', 'pipe', '(', 'or', 'disjunction', ')', 'symbol', 'like', 'thi', ':', '«', 'w', '(', 'i|e|ai|oo', ')', 't', '»', ',', 'match', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'It', 'i', 'instruct', 'to', 'see', 'what', 'happen', 'when', 'you', 'omit', 'the', 'parenthes', 'from', 'the', 'last', 'expression', 'above', '.']\n",
      "\n",
      "Word stemming using porter stemmer :\n",
      "\n",
      "['you', 'probabl', 'work', 'out', 'that', 'a', 'backslash', 'mean', 'that', 'the', 'follow', 'charact', 'is', 'depriv', 'of', 'it', 'special', 'power', 'and', 'must', 'liter', 'match', 'a', 'specif', 'charact', 'in', 'the', 'word', '.', 'thu', ',', 'while', '.', 'is', 'special', ',', '\\\\', '.', 'onli', 'match', 'a', 'period', '.', 'the', 'brace', 'express', ',', 'like', '{', '3,5', '}', ',', 'specifi', 'the', 'number', 'of', 'repeat', 'of', 'the', 'previou', 'item', '.', 'the', 'pipe', 'charact', 'indic', 'a', 'choic', 'between', 'the', 'materi', 'on', 'it', 'left', 'or', 'it', 'right', '.', 'parenthes', 'indic', 'the', 'scope', 'of', 'an', 'oper', ':', 'they', 'can', 'be', 'use', 'togeth', 'with', 'the', 'pipe', '(', 'or', 'disjunct', ')', 'symbol', 'like', 'thi', ':', '«', 'w', '(', 'i|e|ai|oo', ')', 't', '»', ',', 'match', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'It', 'is', 'instruct', 'to', 'see', 'what', 'happen', 'when', 'you', 'omit', 'the', 'parenthes', 'from', 'the', 'last', 'express', 'abov', '.']\n",
      "\n",
      "Word stemming using lancaster stemmer :\n",
      "\n",
      "['you', 'prob', 'work', 'out', 'that', 'a', 'backslash', 'mean', 'that', 'the', 'follow', 'charact', 'is', 'depr', 'of', 'it', 'spec', 'pow', 'and', 'must', 'lit', 'match', 'a', 'spec', 'charact', 'in', 'the', 'word', '.', 'thu', ',', 'whil', '.', 'is', 'spec', ',', '\\\\', '.', 'on', 'match', 'a', 'period', '.', 'the', 'brac', 'express', ',', 'lik', '{', '3,5', '}', ',', 'spec', 'the', 'numb', 'of', 'rep', 'of', 'the', 'prevy', 'item', '.', 'the', 'pip', 'charact', 'ind', 'a', 'cho', 'between', 'the', 'mat', 'on', 'it', 'left', 'or', 'it', 'right', '.', 'parenthes', 'ind', 'the', 'scop', 'of', 'an', 'op', ':', 'they', 'can', 'be', 'us', 'togeth', 'with', 'the', 'pip', '(', 'or', 'disjunct', ')', 'symbol', 'lik', 'thi', ':', '«', 'w', '(', 'i|e|ai|oo', ')', 't', '»', ',', 'match', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'it', 'is', 'instruct', 'to', 'see', 'what', 'hap', 'when', 'you', 'omit', 'the', 'parenthes', 'from', 'the', 'last', 'express', 'abov', '.']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# function for stemming using RE\n",
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "file = open(\"Data_1.txt\").read() # read txt file\n",
    "print(\"Word stemming using regular expression :\\n\")\n",
    "tokens = word_tokenize(file)\n",
    "print([stem(t) for t in tokens])\n",
    "\n",
    "# stemming using porter stemmer\n",
    "porter = nltk.PorterStemmer() \n",
    "porterstem=[]\n",
    "print(\"\\nWord stemming using porter stemmer :\\n\")\n",
    "for t in tokens:\n",
    "    result = porter.stem(t)\n",
    "    porterstem.append(result)\n",
    "print(porterstem)\n",
    "\n",
    "# stemming using lancaster stemmer\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "lancasterstem=[]\n",
    "print(\"\\nWord stemming using lancaster stemmer :\\n\")\n",
    "for t in tokens:\n",
    "    result = lancaster.stem(t)\n",
    "    lancasterstem.append(result)\n",
    "print(lancasterstem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text with stop words and punctuations :\n",
      "\n",
      "['you', 'probably', 'worked', 'out', 'that', 'a', 'backslash', 'means', 'that', 'the', 'following', 'character', 'is', 'deprived', 'of', 'its', 'special', 'powers', 'and', 'must', 'literally', 'match', 'a', 'specific', 'character', 'in', 'the', 'word', '.', 'thus', ',', 'while', '.', 'is', 'special', ',', '\\\\', '.', 'only', 'matches', 'a', 'period', '.', 'the', 'braced', 'expressions', ',', 'like', '{', '3,5', '}', ',', 'specify', 'the', 'number', 'of', 'repeats', 'of', 'the', 'previous', 'item', '.', 'the', 'pipe', 'character', 'indicates', 'a', 'choice', 'between', 'the', 'material', 'on', 'its', 'left', 'or', 'its', 'right', '.', 'parentheses', 'indicate', 'the', 'scope', 'of', 'an', 'operator', ':', 'they', 'can', 'be', 'used', 'together', 'with', 'the', 'pipe', '(', 'or', 'disjunction', ')', 'symbol', 'like', 'this', ':', '«', 'w', '(', 'i|e|ai|oo', ')', 't', '»', ',', 'matching', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'it', 'is', 'instructive', 'to', 'see', 'what', 'happens', 'when', 'you', 'omit', 'the', 'parentheses', 'from', 'the', 'last', 'expression', 'above', '.']\n",
      "\n",
      "Total number of words in this text corpus is  138\n",
      "There are 59 words in this text after removing stop words\n",
      "\n",
      "['probably', 'worked', 'backslash', 'means', 'following', 'character', 'deprived', 'special', 'powers', 'must', 'literally', 'match', 'specific', 'character', 'word', 'thus', 'special', 'matches', 'period', 'braced', 'expressions', 'like', '3,5', 'specify', 'number', 'repeats', 'previous', 'item', 'pipe', 'character', 'indicates', 'choice', 'material', 'left', 'right', 'parentheses', 'indicate', 'scope', 'operator', 'used', 'together', 'pipe', 'disjunction', 'symbol', 'like', 'w', 'i|e|ai|oo', 'matching', 'wit', 'wet', 'wait', 'woot', 'instructive', 'see', 'happens', 'omit', 'parentheses', 'last', 'expression']\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read txt file\n",
    "file = open(\"Data_1.txt\").read()\n",
    "file=file.lower()\n",
    "\n",
    "# toeknize the words\n",
    "wordtokens = nltk.tokenize.word_tokenize(file)\n",
    "print('The original text with stop words and punctuations :\\n')\n",
    "print(wordtokens)\n",
    "\n",
    "print(\"\\nTotal number of words in this text corpus is \",len(wordtokens))\n",
    "\n",
    "\n",
    "all_stopwords = nltk.corpus.stopwords.words('english')+ list(string.punctuation)\n",
    "all_stopwords.append('»') #add punctuation\n",
    "all_stopwords.append('«') #add punctuation\n",
    "\n",
    "\n",
    "filteredTokens = [] #declare list\n",
    "\n",
    "# inserting tokens to the list\n",
    "for w in wordtokens:\n",
    "    if w not in all_stopwords :\n",
    "        filteredTokens.append(w)\n",
    "\n",
    "print(\"There are\", len(filteredTokens), \"words in this text after removing stop words\\n\")\n",
    "print(filteredTokens)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 38  stop words in this text\n",
      "\n",
      "['you', 'out', 'that', 'a', 'the', 'is', 'of', 'its', 'and', 'in', '.', ',', 'while', '\\\\', 'only', '{', '}', 'between', 'on', 'or', 'an', ':', 'they', 'can', 'be', 'with', '(', ')', 'this', '«', 't', '»', 'it', 'to', 'what', 'when', 'from', 'above']\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read file\n",
    "file = open(\"Data_1.txt\").read()\n",
    "file=file.lower()\n",
    "wordtokens = nltk.tokenize.word_tokenize(file)\n",
    "all_stopwords = nltk.corpus.stopwords.words('english')+ list(string.punctuation)\n",
    "all_stopwords.append('»')\n",
    "all_stopwords.append('«')\n",
    "stpTokens = [] #declare list\n",
    "\n",
    "# inserting stop words and punctuations into the list\n",
    "for w in wordtokens:\n",
    "    if w in all_stopwords :\n",
    "        if w not in stpTokens:\n",
    "            stpTokens.append(w)\n",
    "\n",
    "print(\"There are\", len(stpTokens), \" stop words in this text\\n\")\n",
    "print(stpTokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos tagging using RE : \n",
      "\n",
      "[('the', 'NN'), ('little', 'NN'), ('yellow', 'NN'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'NN'), ('the', 'NN'), ('cute', 'NN'), ('cat', 'NN'), ('and', 'NN'), ('chased', 'VBD'), ('away', 'NN'), ('.', 'NN')]\n",
      "\n",
      "Pos tagging using Text Blob : \n",
      "\n",
      "[('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cute', 'NN'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB')]\n",
      "\n",
      "Pos tagging using NLTK : \n",
      "\n",
      "[('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cute', 'NN'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# define RE patterns\n",
    "patterns = [\n",
    "     (r'.*ing$', 'VBG'),               # gerunds\n",
    "     (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'.*', 'NN'),                    # nouns (default)\n",
    "     (r'^\\d+$', 'CD'),\n",
    "     (r'.*ing$', 'VBG'),               # gerunds, i.e. wondering\n",
    "     (r'.*ment$', 'NN'),               # i.e. wonderment\n",
    "     (r'.*ful$', 'JJ')                 # i.e. wonderful\n",
    " ]\n",
    "\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "tagger=nltk.tag.sequential.RegexpTagger(patterns)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "#read txt file\n",
    "file = open(\"Data_2.txt\").read()\n",
    "file=file.lower() # change all words to lowercase\n",
    "wordtokens = nltk.tokenize.word_tokenize(file)\n",
    "\n",
    "# pos tagging with RE\n",
    "print(\"Pos tagging using RE : \\n\")\n",
    "print(tagger.tag(wordtokens))\n",
    "\n",
    "#pos tagging using TextBlob\n",
    "wiki = TextBlob(file)\n",
    "print(\"\\nPos tagging using Text Blob : \\n\")\n",
    "print(wiki.tags)\n",
    "\n",
    "#pos tagging using NLTK\n",
    "print(\"\\nPos tagging using NLTK : \\n\")\n",
    "print(nltk.pos_tag(wordtokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#declare the CFG pattern\n",
    "text = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> DT Nom\n",
    "Nom -> Adj Nom | NN VP | NN\n",
    "VP -> V AA | Con VP\n",
    "AA -> PP AA | DT Nom| PP\n",
    "Adj -> 'little' | 'yellow' | 'cute'\n",
    "NN -> 'dog' | 'cat'\n",
    "V -> 'barked' | 'chased'\n",
    "PP -> 'at' | 'away'\n",
    "Con -> 'and'\n",
    "DT -> 'the'\n",
    "\"\"\")\n",
    "#read file\n",
    "file = open(\"Data_2.txt\").read()\n",
    "file = file.lower()\n",
    "\n",
    "all_stopwords = []\n",
    "words=[]\n",
    "all_stopwords.append('.') \n",
    "# tokenize words\n",
    "wordtokens = nltk.tokenize.word_tokenize(file)\n",
    "# delete '.'\n",
    "for w in wordtokens:\n",
    "    if w not in all_stopwords :\n",
    "        words.append(w)\n",
    "# create parse tree\n",
    "parser = nltk.ChartParser(text)\n",
    "for tree in parser.parse(words):\n",
    "    tree.draw()\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
